{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univeral Code Used for the Entire Notebook\n",
    "\n",
    "Let's set up our libraries and client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json  # Handles JSON data encoding and decoding\n",
    "import random  # Generates random numbers and makes random selections\n",
    "import time  # Provides time-related functions\n",
    "import math  # Offers mathematical functions and constants\n",
    "from pathlib import Path  # Handles filesystem paths in an object-oriented way\n",
    "from collections import defaultdict  # Provides a dictionary subclass with default values\n",
    "import base64  # Provides data encoding and decoding as specified in RFC 3548\n",
    "import io  # Offers core tools for working with streams\n",
    "import sys  # Provides access to some variables used or maintained by the interpreter\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np  # Supports large, multi-dimensional arrays and matrices\n",
    "import pandas as pd  # Offers data manipulation and analysis tools\n",
    "import tiktoken  # Handles tokenization for OpenAI models\n",
    "from openai import OpenAI, RateLimitError  # OpenAI API client and related error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client\n",
    "client = OpenAI()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Files Validation & Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and print the dataset\n",
    "def load_and_print_dataset(data_path):\n",
    "    \"\"\"\n",
    "    Load the dataset from a given file path and print initial statistics.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the dataset file.\n",
    "        \n",
    "    Returns:\n",
    "        dataset (list): Loaded dataset as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        dataset = [json.loads(line) for line in file]\n",
    "    \n",
    "    # Print initial dataset statistics\n",
    "    print(\"Number of examples:\", len(dataset))\n",
    "    print(\"First example:\")\n",
    "    \n",
    "    # Print messages from the first example in the dataset\n",
    "    for message in dataset[0][\"messages\"]:\n",
    "        print(message)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function to load and print the dataset\n",
    "data_path = \"./artifacts/marv_fine_tune.jsonl\"\n",
    "dataset = load_and_print_dataset(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format Validation\n",
    "\n",
    "We can perform a variety of error checks to validate that each conversation in the dataset adheres to the format expected by the fine-tuning API. Errors are categorized based on their nature for easier debugging.\n",
    "\n",
    "1. **Data Type Check**: Checks whether each entry in the dataset is a dictionary (dict). Error type: `data_type`.\n",
    "\n",
    "2. **Presence of Message List**: Checks if a `messages` list is present in each entry. Error type: `missing_messages_list`.\n",
    "\n",
    "3. **Message Keys Check**: Validates that each message in the `messages` list contains the keys `role` and `content`. Error type: `message_missing_key`.\n",
    "\n",
    "4. **Unrecognized Keys in Messages**: Logs if a message has keys other than `role`, `content`, `weight`, `function_call`, and `name`. Error type: `message_unrecognized_key`.\n",
    "\n",
    "5. **Role Validation**: Ensures the `role` is one of \"system\", \"user\", or \"assistant\". Error type: `unrecognized_role`.\n",
    "\n",
    "6. **Content Validation**: Verifies that `content` has textual data and is a string. Error type: `missing_content`.\n",
    "\n",
    "7. **Assistant Message Presence**: Checks that each conversation has at least one message from the assistant. Error type: `example_missing_assistant_message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for format errors in our file\n",
    "def check_format_errors(dataset):\n",
    "    \"\"\"\n",
    "    Check for format errors in the dataset and print the results.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): The dataset to check.\n",
    "        \n",
    "    Returns:\n",
    "        format_errors (dict): A dictionary containing the count of each type of format error.\n",
    "    \"\"\"\n",
    "    # Dictionary to track format errors\n",
    "    format_errors = defaultdict(int)\n",
    "    \n",
    "    # Iterate through each example in the dataset\n",
    "    for ex in dataset:\n",
    "        # Check if the example is a dictionary\n",
    "        if not isinstance(ex, dict):\n",
    "            format_errors[\"data_type\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Retrieve the messages list from the example\n",
    "        messages = ex.get(\"messages\", None)\n",
    "        if not messages:\n",
    "            format_errors[\"missing_messages_list\"] += 1\n",
    "            continue\n",
    "        \n",
    "        # Check each message in the messages list\n",
    "        for message in messages:\n",
    "            # Check if required keys are present in the message\n",
    "            if \"role\" not in message or \"content\" not in message:\n",
    "                format_errors[\"message_missing_key\"] += 1\n",
    "            \n",
    "            # Check for any unrecognized keys in the message\n",
    "            if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "                format_errors[\"message_unrecognized_key\"] += 1\n",
    "            \n",
    "            # Validate the role value in the message\n",
    "            if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "                format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "            # Check content and function_call in the message\n",
    "            content = message.get(\"content\", None)\n",
    "            function_call = message.get(\"function_call\", None)\n",
    "            if (not content and not function_call) or not isinstance(content, str):\n",
    "                format_errors[\"missing_content\"] += 1\n",
    "        \n",
    "        # Ensure at least one message from the assistant is present\n",
    "        if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "            format_errors[\"example_missing_assistant_message\"] += 1\n",
    "    \n",
    "    # Print the results of the error checks\n",
    "    if format_errors:\n",
    "        print(\"Found possible issues:\")\n",
    "        for key, value in format_errors.items():\n",
    "            print(f\"{key}: {value}\")\n",
    "    else:\n",
    "        print(\"No errors found\")\n",
    "    \n",
    "    return format_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function to check for format errors\n",
    "format_errors = check_format_errors(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Counting Utilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Warnings, Token Counts, and Cost Estimation\n",
    "\n",
    "With some lightweight analysis we can identify potential issues in the dataset, like missing messages, and provide statistical insights into message and token counts.\n",
    "\n",
    "1. **Missing System/User Messages**: Counts the number of conversations missing a \"system\" or \"user\" message. Such messages are critical for defining the assistant's behavior and initiating the conversation.\n",
    "\n",
    "2. **Number of Messages Per Example**: Summarizes the distribution of the number of messages in each conversation, providing insight into dialogue complexity.\n",
    "\n",
    "3. **Total Tokens Per Example**: Calculates and summarizes the distribution of the total number of tokens in each conversation. Important for understanding fine-tuning costs.\n",
    "\n",
    "4. **Tokens in Assistant's Messages**: Calculates the number of tokens in the assistant's messages per conversation and summarizes this distribution. Useful for understanding the assistant's verbosity.\n",
    "\n",
    "5. **Token Limit Warnings**: Checks if any examples exceed the maximum token limit (16,385 tokens), as such examples will be truncated during fine-tuning, potentially resulting in data loss.\n",
    "\n",
    "\n",
    "Finally, we estimate the total number of tokens that will be used for fine-tuning, which allows us to approximate the cost. It is worth noting that the duration of the fine-tuning jobs will also increase with the token count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MAX_TOKENS_PER_EXAMPLE = 640\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 10\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "# Automatically get the encoding for a specific model\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "\n",
    "def process_dataset(dataset, num_tokens_from_messages,\n",
    "                    num_assistant_tokens_from_messages, token_limit=64000):\n",
    "    \"\"\"\n",
    "    Process the dataset and calculate various statistics.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of examples in the dataset.\n",
    "        num_tokens_from_messages (function): Function to count tokens in messages.\n",
    "        num_assistant_tokens_from_messages (function): Function to count assistant tokens.\n",
    "        token_limit (int): Maximum token limit for conversations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Contains lists of message counts, conversation lengths, and assistant message lengths.\n",
    "    \"\"\"\n",
    "    n_missing_system = 0\n",
    "    n_missing_user = 0\n",
    "    n_messages = []\n",
    "    convo_lens = []\n",
    "    assistant_message_lens = []\n",
    "\n",
    "    for i, ex in enumerate(dataset):\n",
    "        messages = ex[\"messages\"]\n",
    "        if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "            n_missing_system += 1\n",
    "        if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "            n_missing_user += 1\n",
    "        n_messages.append(len(messages))\n",
    "        try:\n",
    "            convo_lens.append(num_tokens_from_messages(messages))\n",
    "            assistant_message_lens.append(\n",
    "                num_assistant_tokens_from_messages(messages)\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}:\")\n",
    "            print(f\"Messages: {messages}\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    n_too_long = sum(l > token_limit for l in convo_lens)\n",
    "\n",
    "    print_summary(n_missing_system, n_missing_user, n_messages,\n",
    "                convo_lens, assistant_message_lens, n_too_long, token_limit)\n",
    "    return n_messages, convo_lens, assistant_message_lens\n",
    "\n",
    "\n",
    "def print_summary(n_missing_system, n_missing_user, n_messages,\n",
    "                convo_lens, assistant_message_lens, n_too_long, token_limit):\n",
    "    \"\"\"\n",
    "    Print a summary of the dataset processing results.\n",
    "\n",
    "    Args:\n",
    "        n_missing_system (int): Number of examples missing system messages.\n",
    "        n_missing_user (int): Number of examples missing user messages.\n",
    "        n_messages (list): List of message counts for each example.\n",
    "        convo_lens (list): List of conversation lengths in tokens.\n",
    "        assistant_message_lens (list): List of assistant message lengths in tokens.\n",
    "        n_too_long (int): Number of conversations exceeding the token limit.\n",
    "        token_limit (int): Maximum token limit for conversations.\n",
    "    \"\"\"\n",
    "    print(\"Summary of dataset processing:\")\n",
    "    print(f\"Num examples missing system message: {n_missing_system}\")\n",
    "    print(f\"Num examples missing user message: {n_missing_user}\")\n",
    "    print(f\"Total number of examples: {len(n_messages)}\")\n",
    "    print(f\"Average number of messages per example: \"\n",
    "        f\"{sum(n_messages) / len(n_messages):.2f}\")\n",
    "    print(f\"Average number of total tokens per example: \"\n",
    "        f\"{sum(convo_lens) / len(convo_lens):.2f}\")\n",
    "    print(f\"Average number of assistant tokens per example: \"\n",
    "        f\"{sum(assistant_message_lens) / len(assistant_message_lens):.2f}\")\n",
    "    print(f\"{n_too_long} examples may be over the {token_limit} token limit \"\n",
    "        f\"and will be truncated during fine-tuning\")\n",
    "\n",
    "\n",
    "def calculate_epochs(n_train_examples):\n",
    "    \"\"\"\n",
    "    Calculate the number of epochs based on the number of training examples.\n",
    "\n",
    "    Args:\n",
    "        n_train_examples (int): Number of training examples.\n",
    "\n",
    "    Returns:\n",
    "        int: Calculated number of epochs.\n",
    "    \"\"\"\n",
    "    if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "        return min(MAX_DEFAULT_EPOCHS,\n",
    "                math.ceil(MIN_TARGET_EXAMPLES / n_train_examples))\n",
    "    elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "        return max(MIN_DEFAULT_EPOCHS,\n",
    "                   MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "    return TARGET_EPOCHS\n",
    "\n",
    "\n",
    "def calculate_billing_tokens(convo_lens):\n",
    "    \"\"\"\n",
    "    Calculate the number of billing tokens in the dataset.\n",
    "\n",
    "    Args:\n",
    "        convo_lens (list): List of conversation lengths in tokens.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of billing tokens.\n",
    "    \"\"\"\n",
    "    return sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "\n",
    "\n",
    "def print_dataset_statistics(n_train_examples, convo_lens):\n",
    "    \"\"\"\n",
    "    Print the dataset statistics and billing information.\n",
    "\n",
    "    Args:\n",
    "        n_train_examples (int): Number of training examples.\n",
    "        convo_lens (list): List of conversation lengths in tokens.\n",
    "    \"\"\"\n",
    "    n_epochs = calculate_epochs(n_train_examples)\n",
    "    n_billing_tokens = calculate_billing_tokens(convo_lens)\n",
    "\n",
    "    print(f\"Dataset Statistics:\")\n",
    "    print(f\"- Number of training examples: {n_train_examples}\")\n",
    "    print(f\"- Approximate billable tokens: {n_billing_tokens}\")\n",
    "    print(f\"- Default number of epochs: {n_epochs}\")\n",
    "    print(f\"- Estimated total billable tokens: {n_epochs * n_billing_tokens}\")\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in a list of messages.\n",
    "\n",
    "    Args:\n",
    "        messages (list): List of message dictionaries.\n",
    "        tokens_per_message (int): Base tokens per message.\n",
    "        tokens_per_name (int): Additional tokens for the 'name' field.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of tokens.\n",
    "    \"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            if key == \"content\" and value is None:\n",
    "                continue\n",
    "            elif key == \"function_call\":\n",
    "                num_tokens += len(encoding.encode(json.dumps(value)))\n",
    "            else:\n",
    "                try:\n",
    "                    num_tokens += len(encoding.encode(str(value)))\n",
    "                except Exception as e:\n",
    "                    print(f\"Error encoding key: {key}, value: {value}, \"\n",
    "                        f\"type: {type(value)}\")\n",
    "                    print(f\"Error message: {str(e)}\")\n",
    "                    raise\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # Adding 3 tokens for end of sequence\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    \"\"\"\n",
    "    Calculate the number of tokens in assistant messages.\n",
    "\n",
    "    Args:\n",
    "        messages (list): List of message dictionaries.\n",
    "\n",
    "    Returns:\n",
    "        int: Total number of tokens in assistant messages.\n",
    "    \"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            if message.get(\"content\") is not None:\n",
    "                num_tokens += len(encoding.encode(str(message[\"content\"])))\n",
    "            if \"function_call\" in message:\n",
    "                num_tokens += len(encoding.encode(json.dumps(message[\"function_call\"])))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset and extract relevant information\n",
    "n_messages, convo_lens, assistant_message_lens = process_dataset(\n",
    "    dataset,\n",
    "    num_tokens_from_messages,\n",
    "    num_assistant_tokens_from_messages\n",
    ")\n",
    "\n",
    "# Get the total number of examples in the dataset\n",
    "n_train_examples = len(dataset)\n",
    "\n",
    "# Print statistics about the dataset\n",
    "print_dataset_statistics(n_train_examples, convo_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Test Split Function for JSONL Files\n",
    "def split_jsonl_file(file_path, train_ratio=0.8):\n",
    "    # Read the input file\n",
    "    file_path = Path(file_path)\n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculate split index\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    \n",
    "    # Split the data\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    # Prepare output file paths\n",
    "    train_file = file_path.with_name(f\"{file_path.stem}_train{file_path.suffix}\")\n",
    "    test_file = file_path.with_name(f\"{file_path.stem}_test{file_path.suffix}\")\n",
    "    \n",
    "    # Write train data\n",
    "    with train_file.open('w', encoding='utf-8') as f:\n",
    "        for item in train_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # Write test data\n",
    "    with test_file.open('w', encoding='utf-8') as f:\n",
    "        for item in test_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Train data saved to: {train_file}\")\n",
    "    print(f\"Test data saved to: {test_file}\")\n",
    "    print(f\"Train set size: {len(train_data)}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    \n",
    "    return(train_file, test_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and data processing\n",
    "file_path = \"./artifacts/marv_fine_tune.jsonl\"\n",
    "\n",
    "# Split the JSONL file into train and test sets\n",
    "train_test_files = split_jsonl_file(file_path)\n",
    "print(\"\\n\")  # Print a blank line for better output readability\n",
    "\n",
    "# Convert the returned file paths to strings\n",
    "train_path, test_path = [str(file) for file in train_test_files]\n",
    "\n",
    "# Print the paths of the resulting train and test files\n",
    "print(f\"Train file path: {train_path}\")\n",
    "print(f\"Test file path: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Fine-Tuning Job\n",
    "\n",
    "### Uploading Training and Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data to the OpenAI API\n",
    "train__set_file = client.files.create(\n",
    "            file=open(train_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )\n",
    "\n",
    "# Upload the test data to the OpenAI API\n",
    "test_set_file = client.files.create(\n",
    "            file=open(test_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple Fine-Tuning Job (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "simple_ft_job = client.fine_tuning.jobs.create(\n",
    "    training_file=train__set_file.id, \n",
    "    model=\"gpt-4o-mini-2024-07-18\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Fine-Tuning Job with All Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "all_params_ft_job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",  # Base model to be fine-tuned\n",
    "    training_file=train__set_file.id,  # ID of the uploaded training data file\n",
    "    validation_file=test_set_file.id,  # ID of the uploaded validation (test) data file\n",
    "    hyperparameters={\n",
    "        \"batch_size\": \"auto\",  # Let API automatically determine batch size\n",
    "        \"learning_rate_multiplier\": \"auto\",  # Auto-set learning rate multiplier\n",
    "        \"n_epochs\": \"auto\",  # Automatically decide number of training epochs\n",
    "    },\n",
    "    suffix=\"marv_ft_0003\",  # Append this to the fine-tuned model's name\n",
    "    integrations=None,  # Specific integrations used\n",
    "    seed=None,  # Specific random seed set for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Training Metrics\n",
    "\n",
    "### Pulling Training Metrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This set of code will check the status of the fine-tuning job\n",
    "# Repeating the check every 60 seconds until the job is done\n",
    "\n",
    "class FineTuningFailedException(Exception):\n",
    "    \"\"\"Custom exception for failed fine-tuning jobs.\"\"\"\n",
    "    pass\n",
    "\n",
    "def check_fine_tuning_status(client, job_id):\n",
    "    \"\"\"\n",
    "    Continuously check the status of a fine-tuning job until it succeeds or fails.\n",
    "\n",
    "    Args:\n",
    "        client: The API client object.\n",
    "        job_id: The ID of the fine-tuning job to check.\n",
    "\n",
    "    Returns:\n",
    "        The final job details if the job succeeds.\n",
    "\n",
    "    Raises:\n",
    "        FineTuningFailedException: If the fine-tuning job fails.\n",
    "        Exception: For any other errors during the process.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Retrieve updated information for the fine-tuning job\n",
    "            retrieved_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "            \n",
    "            print(f\"Current status: {retrieved_job.status}\")\n",
    "            \n",
    "            if retrieved_job.status == \"failed\":\n",
    "                print(\"Job failed. Final job details:\")\n",
    "                print(retrieved_job)\n",
    "                raise FineTuningFailedException(\"The fine-tuning job has failed.\")\n",
    "            \n",
    "            if retrieved_job.status == \"succeeded\":\n",
    "                print(\"Job succeeded. Final job details:\")\n",
    "                print(retrieved_job)\n",
    "                return retrieved_job\n",
    "            \n",
    "            # Wait for 60 seconds before checking again\n",
    "            time.sleep(60)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise  # Re-raise the exception to stop the function\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the check_fine_tuning_status function\n",
    "\n",
    "# Extract the job ID from the created fine-tuning job\n",
    "job_id = all_params_ft_job.id\n",
    "\n",
    "# Monitor the fine-tuning job status until completion\n",
    "final_job = check_fine_tuning_status(client, job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the training metrics for a fine-tuning job\n",
    "def fetch_and_process_fine_tuning_metrics(client, fine_tuning_job_id):\n",
    "    # Function to replace colons with underscores for file names\n",
    "    def replace_colons_with_underscores(input_string):\n",
    "        return input_string.replace(':', '_')\n",
    "    \n",
    "    # Get the training metrics for the fine-tuning job\n",
    "    fine_tune_job = client.fine_tuning.jobs.retrieve(fine_tuning_job_id)\n",
    "        \n",
    "    # File ID of the file you want to download\n",
    "    file_id = fine_tune_job.result_files[0]\n",
    "    \n",
    "    # Retrieve the file content directly\n",
    "    response = client.files.content(file_id)\n",
    "    \n",
    "    # Decode the Base64 content\n",
    "    decoded_content = base64.b64decode(response.content).decode('utf-8')\n",
    "    \n",
    "    # Create a DataFrame from the decoded content\n",
    "    df = pd.read_csv(io.StringIO(decoded_content))\n",
    "        \n",
    "    # Save the CSV file locally:\n",
    "    metrics_file_name = \"step_metrics_\" + replace_colons_with_underscores(fine_tune_job.fine_tuned_model + \".csv\")\n",
    "    df.to_csv(metrics_file_name, index=False)\n",
    "    print(f\"\\nFile saved as {metrics_file_name}\")\n",
    "    \n",
    "    # Return our dataframe in case the caller wants to do more with it\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "fetch_and_process_fine_tuning_metrics(client, all_params_ft_job .id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Fine-Tuning Jobs\n",
    "\n",
    "### List Fine-Tuning Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list our fine-tuning jobs\n",
    "ft_jobs_list = client.fine_tuning.jobs.list()\n",
    "\n",
    "print(ft_jobs_list)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print job IDs, objects, and statuses\n",
    "for job in ft_jobs_list.data:\n",
    "    print(job.id, job.object, job.status)\n",
    "\n",
    "\n",
    "# Print detailed information for only the first job in the list\n",
    "job = ft_jobs_list.data[0]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"\"\"\n",
    "Job ID: {job.id}\n",
    "Created At: {job.created_at}\n",
    "Error: {job.error}\n",
    "Fine-tuned Model: {job.fine_tuned_model}\n",
    "Finished At: {job.finished_at}\n",
    "Hyperparameters:\n",
    "    - Epochs: {job.hyperparameters.n_epochs}\n",
    "    - Batch Size: {job.hyperparameters.batch_size}\n",
    "    - Learning Rate Multiplier: {job.hyperparameters.learning_rate_multiplier}\n",
    "Model: {job.model}\n",
    "Object: {job.object}\n",
    "Organization ID: {job.organization_id}\n",
    "Result Files: {', '.join(str(file) for file in job.result_files)}\n",
    "Seed: {job.seed}\n",
    "Status: {job.status}\n",
    "Trained Tokens: {job.trained_tokens}\n",
    "Training File: {job.training_file}\n",
    "Validation File: {job.validation_file}\n",
    "Estimated Finish: {job.estimated_finish}\n",
    "Integrations: {', '.join(str(integration) for integration in job.integrations) if job.integrations else 'None'}\n",
    "User Provided Suffix: {job.user_provided_suffix}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving Fine-Tuning Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the fine-tuning job by ID\n",
    "retrieved_job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "\n",
    "print(f\"\"\"\n",
    "retrieved_job ID: {retrieved_job.id}\n",
    "Created At: {retrieved_job.created_at}\n",
    "Error: {retrieved_job.error}\n",
    "Fine-tuned Model: {retrieved_job.fine_tuned_model}\n",
    "Finished At: {retrieved_job.finished_at}\n",
    "Hyperparameters:\n",
    "    - Epochs: {retrieved_job.hyperparameters.n_epochs}\n",
    "    - Batch Size: {retrieved_job.hyperparameters.batch_size}\n",
    "    - Learning Rate Multiplier: {retrieved_job.hyperparameters.learning_rate_multiplier}\n",
    "Model: {retrieved_job.model}\n",
    "Object: {retrieved_job.object}\n",
    "Organization ID: {retrieved_job.organization_id}\n",
    "Result Files: {', '.join(str(file) for file in job.result_files)}\n",
    "Seed: {retrieved_job.seed}\n",
    "Status: {retrieved_job.status}\n",
    "Trained Tokens: {retrieved_job.trained_tokens}\n",
    "Training File: {retrieved_job.training_file}\n",
    "Validation File: {retrieved_job.validation_file}\n",
    "Estimated Finish: {retrieved_job.estimated_finish}\n",
    "Integrations: {', '.join(str(integration) for integration in job.integrations) if job.integrations else 'None'}\n",
    "User Provided Suffix: {retrieved_job.user_provided_suffix}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Checkpoints\n",
    "\n",
    "### Listing Job Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all fine-tuning job checkpoints\n",
    "ft_jobs_checkpoints_list = client.fine_tuning.jobs.checkpoints.list(all_params_ft_job.id)\n",
    "\n",
    "# Print the entire list of checkpoints\n",
    "print(ft_jobs_checkpoints_list)\n",
    "print(\"\\n\\n\")  # Add two blank lines for better readability\n",
    "\n",
    "# Iterate through each checkpoint and print specific details\n",
    "for checkpoint in ft_jobs_checkpoints_list:\n",
    "    print(\"Checkpoint ID: \", checkpoint.id, \"\\n\",                        # Unique identifier for the checkpoint\n",
    "        \"Fine-tuned Model Checkpoint: \", checkpoint.fine_tuned_model_checkpoint, \"\\n\",  # Checkpoint of the fine-tuned model\n",
    "        \"Step Number: \", checkpoint.step_number, \"\\n\",                # Training step at which checkpoint was saved\n",
    "        \"Created At: \", checkpoint.created_at, \"\\n\",                  # Timestamp of checkpoint creation\n",
    "        \"Training Loss: \", checkpoint.metrics.train_loss, \"\\n\",       # Training loss at this checkpoint\n",
    "        \"Validation Loss: \", checkpoint.metrics.valid_loss, \"\\n\",       # Validation loss at this checkpoint\n",
    "        \"Fine-tuning Job ID: \", checkpoint.fine_tuning_job_id, \"\\n\"   # ID of the associated fine-tuning job\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Fine-tuning Events\n",
    "\n",
    "### List Fine-Tuning Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the events for the first fine-tuning job in our list\n",
    "ft_events_list = client.fine_tuning.jobs.list_events(\n",
    "    fine_tuning_job_id=all_params_ft_job.id,\n",
    "    limit=10  # Limit the number of events to retrieve\n",
    ")\n",
    "\n",
    "# Print the entire list of events\n",
    "print(ft_events_list)\n",
    "print(\"\\n\\n\")  # Add two blank lines for better readability\n",
    "\n",
    "# Iterate through each event and print specific details\n",
    "for event in ft_events_list.data:\n",
    "    print(\"Event ID: \", event.id, \"\\n\",             # Unique identifier for the event\n",
    "        \"Object Type: \", event.object, \"\\n\",      # Type of object (likely 'fine_tuning.job.event')\n",
    "        \"Created At: \", event.created_at, \"\\n\",   # Timestamp when the event was created\n",
    "        \"Level: \", event.level, \"\\n\",             # Importance level of the event (e.g., 'info', 'warning')\n",
    "        \"Type: \", event.type, \"\\n\",               # Type of event\n",
    "        \"Message: \", event.message, \"\\n\"          # Descriptive message about the event\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancelling Fine-Tuning Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "dead_ft_job_walking = client.fine_tuning.jobs.create(\n",
    "        model=\"gpt-3.5-turbo-0125\",\n",
    "        training_file=train__set_file.id, \n",
    "        validation_file=None,\n",
    "        hyperparameters={\n",
    "            \"batch_size\": \"auto\",\n",
    "            \"learning_rate_multiplier\": \"auto\",\n",
    "            \"n_epochs\": \"auto\",\n",
    "        },\n",
    "        suffix=\"dead_walking\",\n",
    "        integrations=None,\n",
    "        seed=None,\n",
    "    )\n",
    "\n",
    "# Cancel the fine-tuning job\n",
    "client.fine_tuning.jobs.cancel(dead_ft_job_walking.id)\n",
    "\n",
    "# Retrieve the fine-tuning job by ID and show the updated status\n",
    "dead_ft_job_walking_status = client.fine_tuning.jobs.retrieve(dead_ft_job_walking.id)\n",
    "\n",
    "print(f\"Job status after cancellation: {dead_ft_job_walking_status.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fine-Tuned Models\n",
    "\n",
    "### Using Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all our fine-tuning jobs\n",
    "ft_jobs_list = client.fine_tuning.jobs.list()\n",
    "\n",
    "# Print job IDs, objects, and statuses for the filtered list\n",
    "print(\"===== All Jobs =====\")\n",
    "for job in ft_jobs_list:\n",
    "    print(job.id, job.object, job.status)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Filter the list to only include jobs with a status of \"succeeded\"\n",
    "succeeded_jobs_list = [job for job in ft_jobs_list.data if job.status == \"succeeded\"]\n",
    "\n",
    "print(\"===== Successful Jobs =====\")\n",
    "\n",
    "# Print job IDs, objects, and statuses for the filtered list\n",
    "for job in succeeded_jobs_list:\n",
    "    print(job.id, job.object, job.status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fine-tuned model to generate a completion\n",
    "completion = client.chat.completions.create(\n",
    "    model=succeeded_jobs_list[0].fine_tuned_model,  # Use the first successful fine-tuned model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is limburger cheese?\"},\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Checkpointed Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all fine-tuning job checkpoints for the first successful job\n",
    "ft_jobs_checkpoints_list = client.fine_tuning.jobs.checkpoints.list(\n",
    "    succeeded_jobs_list[0].id  # Use the ID of the first successful job\n",
    ")\n",
    "\n",
    "# Iterate through each checkpoint and print specific details\n",
    "for checkpoint in ft_jobs_checkpoints_list:\n",
    "    print(\"Checkpoint ID: \", checkpoint.id, \"\\n\",                            # Unique identifier for the checkpoint\n",
    "        \"Fine-tuned Model Checkpoint: \", checkpoint.fine_tuned_model_checkpoint, \"\\n\",  # Checkpoint of the fine-tuned model\n",
    "        \"Step Number: \", checkpoint.step_number, \"\\n\",                    # Training step at which checkpoint was saved\n",
    "        \"Created At: \", checkpoint.created_at, \"\\n\",                      # Timestamp of checkpoint creation\n",
    "        \"Training Loss: \", checkpoint.metrics.train_loss, \"\\n\",           # Training loss at this checkpoint\n",
    "        \"Fine-tuning Job ID: \", checkpoint.fine_tuning_job_id, \"\\n\"       # ID of the associated fine-tuning job\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first checkpoint from the paginated results\n",
    "first_checkpoint = next(iter(ft_jobs_checkpoints_list), None)\n",
    "\n",
    "if first_checkpoint:\n",
    "    # Use the checkpointed model to generate a completion\n",
    "    completion = client.chat.completions.create(\n",
    "        model=first_checkpoint.fine_tuned_model_checkpoint,  # Use the checkpointed model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What is limburger cheese?\"},\n",
    "        ]\n",
    "    )\n",
    "    # Print the generated response\n",
    "    print(completion.choices[0].message.content)\n",
    "else:\n",
    "    print(\"No checkpoints found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Fine-Tuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a fine-tuned model\n",
    "# Note: This code is currently commented out\n",
    "\n",
    "# response = client.models.delete(\"ft:gpt-4o-mini-2024-07-18:personal::9rCgzkVh\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Use Cases\n",
    "\n",
    "### Structured Output: Sports Headlines\n",
    "\n",
    "#### Validating Our Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the data file that will be used for fine-tuning\n",
    "\n",
    "# Path to the JSONL data file\n",
    "data_path = \"./artifacts/sports_headlines.jsonl\"\n",
    "\n",
    "# Load and print the dataset\n",
    "dataset = load_and_print_dataset(data_path)\n",
    "print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "# Check for format errors in the dataset\n",
    "format_errors = check_format_errors(dataset)\n",
    "print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "# Process the dataset to extract message counts and token lengths\n",
    "n_messages, convo_lens, assistant_message_lens = process_dataset(\n",
    "    dataset, \n",
    "    num_tokens_from_messages, \n",
    "    num_assistant_tokens_from_messages\n",
    ")\n",
    "print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "# Get the total number of examples in the dataset\n",
    "n_train_examples = len(dataset)\n",
    "print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "# Print statistics about the dataset\n",
    "print_dataset_statistics(n_train_examples, convo_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the JSONL file into training and testing sets\n",
    "train_test_files = split_jsonl_file(data_path)\n",
    "print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "# Convert file paths to strings\n",
    "train_path, test_path = [str(file) for file in train_test_files]\n",
    "\n",
    "# Print the paths of the resulting train and test files\n",
    "print(f\"Train file path: {train_path}\")\n",
    "print(f\"Test file path: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload Our Training and Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data to the OpenAI API\n",
    "sports_headlines_train_file = client.files.create(\n",
    "            file=open(train_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )\n",
    "\n",
    "# Upload the training data to the OpenAI API\n",
    "sports_headlines_test_file = client.files.create(\n",
    "            file=open(test_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit Our Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "sports_headlines_ft_job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",  # Base model to be fine-tuned\n",
    "    training_file=sports_headlines_train_file.id,  # ID of the uploaded training data file\n",
    "    validation_file=sports_headlines_test_file.id,  # ID of the uploaded validation (test) data file\n",
    "    hyperparameters={\n",
    "        \"batch_size\": \"auto\",  # Let API automatically determine batch size\n",
    "        \"learning_rate_multiplier\": \"auto\",  # Auto-set learning rate multiplier\n",
    "        \"n_epochs\": \"auto\",  # Automatically decide number of training epochs\n",
    "    },\n",
    "    suffix=\"sports_headlines\",  # Append this to the fine-tuned model's name\n",
    "    integrations=None,  # No specific integrations used\n",
    "    seed=None,  # No specific random seed set for reproducibility\n",
    ")\n",
    "print(\"Fine-tuning job created successfully!\")\n",
    "print(sports_headlines_ft_job)\n",
    "keep_running = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and Print Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status of the fine-tuning job\n",
    "# Continue checking until the job is complete or fails\n",
    "job_id = sports_headlines_ft_job.id  \n",
    "final_job = check_fine_tuning_status(client, job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and process metrics for the sports headlines fine-tuning job\n",
    "fetch_and_process_fine_tuning_metrics(client, sports_headlines_ft_job.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the New Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the fine-tuning job by ID\n",
    "retrieved_job = client.fine_tuning.jobs.retrieve(sports_headlines_ft_job.id)\n",
    "\n",
    "# Print detailed information about the retrieved job\n",
    "print(f\"\"\"\n",
    "Retrieved Job Details:\n",
    "----------------------\n",
    "Job ID: {retrieved_job.id}\n",
    "Status: {retrieved_job.status}\n",
    "Model: {retrieved_job.model}\n",
    "Created At: {retrieved_job.created_at}\n",
    "Finished At: {retrieved_job.finished_at or 'Not finished yet'}\n",
    "Fine-tuned Model: {retrieved_job.fine_tuned_model or 'Not available yet'}\n",
    "Organization ID: {retrieved_job.organization_id}\n",
    "Result Files: {', '.join(retrieved_job.result_files) if retrieved_job.result_files else 'None'}\n",
    "Trained Tokens: {retrieved_job.trained_tokens or 'Not available'}\n",
    "Hyperparameters:\n",
    "    - Epochs: {retrieved_job.hyperparameters.n_epochs}\n",
    "    - Batch Size: {retrieved_job.hyperparameters.batch_size}\n",
    "    - Learning Rate Multiplier: {retrieved_job.hyperparameters.learning_rate_multiplier}\n",
    "Training File: {retrieved_job.training_file}\n",
    "Validation File: {retrieved_job.validation_file}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fine-tuned model to generate a completion\n",
    "try:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=retrieved_job.fine_tuned_model,  # Use the fine-tuned model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Given a sports headline, provide the following fields in a JSON dict, where applicable: \\\"player\\\" (full name), \\\"team\\\", \\\"sport\\\", and \\\"gender\\\".\"},\n",
    "            {\"role\": \"user\", \"content\": \"2024 Olympics: Biles earns 7th gold in dominant fashion\"},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Print the generated response\n",
    "    print(\"Generated Response:\")\n",
    "    print(completion.choices[0].message.content)\n",
    "    \n",
    "    # Attempt to parse and pretty-print the JSON response\n",
    "    try:\n",
    "        json_response = json.loads(completion.choices[0].message.content)\n",
    "        print(\"\\nFormatted JSON Response:\")\n",
    "        print(json.dumps(json_response, indent=2))\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"\\nNote: The response is not in valid JSON format.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while generating the completion: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Calling: Weather Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the JSONL data file\n",
    "data_path = \"./artifacts/get_current_weather.jsonl\"\n",
    "\n",
    "try:\n",
    "    # Load and print the dataset\n",
    "    dataset = load_and_print_dataset(data_path)\n",
    "    print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "    # Check for format errors in the dataset\n",
    "    format_errors = check_format_errors(dataset)\n",
    "    print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "    # Process the dataset to extract message counts and token lengths\n",
    "    n_messages, convo_lens, assistant_message_lens = process_dataset(\n",
    "        dataset, \n",
    "        num_tokens_from_messages, \n",
    "        num_assistant_tokens_from_messages\n",
    "    )\n",
    "    print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "    # Get the total number of examples in the dataset\n",
    "    n_train_examples = len(dataset)\n",
    "\n",
    "    # Print statistics about the dataset\n",
    "    print_dataset_statistics(n_train_examples, convo_lens)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {data_path} was not found.\")\n",
    "    sys.exit(1)\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: The file {data_path} is not a valid JSON Lines file.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the original JSONL data file\n",
    "data_path = \"./artifacts/get_current_weather.jsonl\"\n",
    "\n",
    "try:\n",
    "    # Split the JSONL file into training and testing sets\n",
    "    train_test_files = split_jsonl_file(data_path)\n",
    "    print(\"\\n\")  # Add a blank line for better readability\n",
    "\n",
    "    # Convert file paths to strings\n",
    "    train_path, test_path = [str(file) for file in train_test_files]\n",
    "\n",
    "    # Print the paths of the resulting train and test files\n",
    "    print(f\"Train file path: {train_path}\")\n",
    "    print(f\"Test file path: {test_path}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {data_path} was not found.\")\n",
    "    sys.exit(1)\n",
    "except ValueError as e:\n",
    "    print(f\"Error in splitting the file: {str(e)}\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Upload the Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data to the OpenAI API\n",
    "get_current_weather_train_file = client.files.create(\n",
    "            file=open(\"./artifacts/get_current_weather_train.jsonl\", \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            ) \n",
    "\n",
    "# Upload the test data to the OpenAI API\n",
    "get_current_data_test_file = client.files.create(\n",
    "            file=open(\"./artifacts/get_current_weather_test.jsonl\", \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking every two hours for the the job to be done\n",
    "retry_interval=7200 # every 2 hours\n",
    "attempt = 1\n",
    "while True:\n",
    "    try:\n",
    "        get_current_weather_ft_job = client.fine_tuning.jobs.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            training_file=get_current_weather_train_file.id, \n",
    "            validation_file=get_current_data_test_file.id,\n",
    "            hyperparameters={\n",
    "                \"batch_size\": \"auto\",\n",
    "                \"learning_rate_multiplier\": \"auto\",\n",
    "                \"n_epochs\": \"auto\",\n",
    "            },\n",
    "            suffix=\"get-weather\",\n",
    "            integrations=None,\n",
    "            seed=None,\n",
    "        )\n",
    "        print(\"Fine-tuning job created successfully!\")\n",
    "    except RateLimitError as e:\n",
    "        print(f\"Attempt {attempt}: Rate limit exceeded. Retrying in {retry_interval//3600} hours...\")\n",
    "        time.sleep(retry_interval)\n",
    "        attempt += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Attempt {attempt}: An unexpected error occurred: {str(e)}\")\n",
    "        print(f\"Retrying in {retry_interval//3600} hours...\")\n",
    "        time.sleep(retry_interval)\n",
    "        attempt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_current_weather_ft_job = client.fine_tuning.jobs.create(\n",
    "            model=\"gpt-4o-mini-2024-07-18\",\n",
    "            training_file=get_current_weather_train_file.id, \n",
    "            validation_file=get_current_data_test_file.id,\n",
    "            hyperparameters={\n",
    "                \"batch_size\": \"auto\",\n",
    "                \"learning_rate_multiplier\": \"auto\",\n",
    "                \"n_epochs\": \"auto\",\n",
    "            },\n",
    "            suffix=\"get-weather\",\n",
    "            integrations=None,\n",
    "            seed=None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: Check the status of the fine-tuning job\n",
    "try:\n",
    "    # Extract the job ID from the previously created fine-tuning job\n",
    "    job_id = get_current_weather_ft_job.id\n",
    "    print(f\"Starting to monitor fine-tuning job with ID: {job_id}\")\n",
    "\n",
    "    # Check the fine-tuning status until completion or failure\n",
    "    final_job = check_fine_tuning_status(client, job_id)\n",
    "    \n",
    "    # Print the final job details\n",
    "    print(\"\\nFinal job details:\")\n",
    "    print(f\"Status: {final_job.status}\")\n",
    "    print(f\"Created at: {final_job.created_at}\")\n",
    "    print(f\"Finished at: {final_job.finished_at}\")\n",
    "    print(f\"Fine-tuned model: {final_job.fine_tuned_model}\")\n",
    "    # You can add more job details here if needed\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Error: 'job' object does not have 'id' attribute. \"\n",
    "        \"Make sure the job was created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Fine-tuning status check process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to simulate getting the current weather for a location\n",
    "def get_current_weather(location: str, format: str = \"celsius\"):\n",
    "    \"\"\"\n",
    "    Get the current weather for a given location.\n",
    "\n",
    "    Args:\n",
    "    location (str): The location to get weather for.\n",
    "    format (str): The temperature format, either \"celsius\" or \"fahrenheit\". Defaults to \"celsius\".\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing weather information.\n",
    "    \"\"\"\n",
    "    # List of possible weather conditions\n",
    "    weather_conditions = [\"sunny\", \"partly cloudy\", \"cloudy\", \"rainy\", \"stormy\"]\n",
    "    \n",
    "    # Generate random temperature and humidity\n",
    "    temperature = random.uniform(-10, 35)\n",
    "    humidity = random.randint(30, 90)\n",
    "    \n",
    "    # Convert temperature to Fahrenheit if requested\n",
    "    if format.lower() == \"fahrenheit\":\n",
    "        temperature = (temperature * 9/5) + 32\n",
    "    \n",
    "    # Prepare and return the weather data\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": round(temperature, 1),\n",
    "        \"unit\": \"°F\" if format.lower() == \"fahrenheit\" else \"°C\",\n",
    "        \"humidity\": humidity,\n",
    "        \"condition\": random.choice(weather_conditions)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the details of a specific fine-tuning job\n",
    "\n",
    "try:\n",
    "    # Retrieve the fine-tuning job by its ID\n",
    "    retrieved_job = client.fine_tuning.jobs.retrieve(get_current_weather_ft_job.id)\n",
    "\n",
    "    # Print detailed information about the retrieved job\n",
    "    print(\"Retrieved Job Details:\")\n",
    "    print(f\"Job ID: {retrieved_job.id}\")\n",
    "    print(f\"Status: {retrieved_job.status}\")\n",
    "    print(f\"Model: {retrieved_job.model}\")\n",
    "    print(f\"Created At: {retrieved_job.created_at}\")\n",
    "    print(f\"Finished At: {retrieved_job.finished_at or 'Not finished yet'}\")\n",
    "    print(f\"Fine-tuned Model: {retrieved_job.fine_tuned_model or 'Not available yet'}\")\n",
    "    print(f\"Organization ID: {retrieved_job.organization_id}\")\n",
    "    print(f\"Result Files: {', '.join(retrieved_job.result_files) if retrieved_job.result_files else 'None'}\")\n",
    "    print(f\"Status: {retrieved_job.status}\")\n",
    "    print(f\"Trained Tokens: {retrieved_job.trained_tokens}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    print(f\"  - Epochs: {retrieved_job.hyperparameters.n_epochs}\")\n",
    "    print(f\"  - Batch Size: {retrieved_job.hyperparameters.batch_size}\")\n",
    "    print(f\"  - Learning Rate Multiplier: {retrieved_job.hyperparameters.learning_rate_multiplier}\")\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Error: 'job' object does not have 'id' attribute. Make sure the job was created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while retrieving the job: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weather_response(user_input):\n",
    "    print(\"Step 1: Initial API call for function calling\")\n",
    "    # First API call to get the function call\n",
    "    response = client.chat.completions.create(\n",
    "        model=retrieved_job.fine_tuned_model,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that can retrieve weather information.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        functions=[{\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather for a specific location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The city and country, e.g., 'London, UK'\"},\n",
    "                    \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use\"}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }],\n",
    "        function_call=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Extract and print function call details\n",
    "    function_call = response.choices[0].message.function_call\n",
    "    print(\"\\nFunction Call Details:\")\n",
    "    print(f\"Function Name: {function_call.name}\")\n",
    "    print(f\"Arguments: {function_call.arguments}\")\n",
    "    \n",
    "    print(\"\\nStep 2: Simulating weather data retrieval\")\n",
    "    # Here you would actually call your weather API with these arguments\n",
    "    # For this example, let's simulate a weather response\n",
    "    function_args = json.loads(function_call.arguments)\n",
    "    weather_data = get_current_weather(\n",
    "        location=function_args['location'],\n",
    "        format=function_args.get('format', 'celsius')\n",
    "    )\n",
    "    print(f\"Weather Data: {json.dumps(weather_data, indent=2)}\")\n",
    "    \n",
    "    print(\"\\nStep 3: Second API call for human-readable response\")\n",
    "    # Second API call to get the final response\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=retrieved_job.fine_tuned_model,  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that can retrieve weather information.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input},\n",
    "            {\"role\": \"assistant\", \"content\": None, \"function_call\": function_call},\n",
    "            {\"role\": \"function\", \"name\": \"get_current_weather\", \"content\": json.dumps(weather_data)}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return final_response.choices[0].message.content\n",
    "\n",
    "# Example usage\n",
    "user_query = \"What's the weather like in Tokyo?\"\n",
    "print(f\"User Query: {user_query}\\n\")\n",
    "response = get_weather_response(user_query)\n",
    "print(\"\\nFinal Response:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Our Train/Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths and data processing\n",
    "file_path = \"./artifacts/marv_fine_tune.jsonl\"\n",
    "\n",
    "# Split the JSONL file into train and test sets\n",
    "wandb_train_test_files = split_jsonl_file(file_path)\n",
    "print(\"\\n\")  # Print a blank line for better output readability\n",
    "\n",
    "# Convert the returned file paths to strings\n",
    "wandb_train_path, wandb_test_path = [str(file) for file in train_test_files]\n",
    "\n",
    "# Print the paths of the resulting train and test files\n",
    "print(f\"Train file path: {wandb_train_path}\")\n",
    "print(f\"Test file path: {wandb_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Train/Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the training data to the OpenAI API\n",
    "wandb_train__set_file = client.files.create(\n",
    "            file=open(wandb_train_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )\n",
    "\n",
    "# Upload the test data to the OpenAI API\n",
    "wandb_test_set_file = client.files.create(\n",
    "            file=open(wandb_test_path, \"rb\"),\n",
    "            purpose=\"fine-tune\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "wandb_params_ft_job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",  # Base model to be fine-tuned\n",
    "    training_file=wandb_train__set_file.id,  # ID of the uploaded training data file\n",
    "    validation_file=wandb_test_set_file.id,  # ID of the uploaded validation (test) data file\n",
    "    hyperparameters={\n",
    "        \"batch_size\": \"auto\",  # Let API automatically determine batch size\n",
    "        \"learning_rate_multiplier\": \"auto\",  # Auto-set learning rate multiplier\n",
    "        \"n_epochs\": \"auto\",  # Automatically decide number of training epochs\n",
    "    },\n",
    "    suffix=\"marv_wandb_tune\",  # Append this to the fine-tuned model's name\n",
    "    integrations=[\n",
    "        {\n",
    "            \"type\": \"wandb\",\n",
    "            \"wandb\": {\n",
    "                \"project\": \"Marv_Fun_Tune_v2\",  # Replace with your actual project name\n",
    "                \"name\": \"Marv_run_001\",  # Optional: Replace with your desired run name or remove\n",
    "                \"entity\": \"suspicious-cow-self\",  # Optional: Replace with your entity or remove\n",
    "                \"tags\": [\"rando_tag1\", \"rando_tag2\"]  # Optional: Replace with your desired tags or remove\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    seed=None,  # Specific random seed set for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check to Make Sure the Job is Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: Check the status of the fine-tuning job\n",
    "try:\n",
    "    # Extract the job ID from the previously created fine-tuning job\n",
    "    job_id = wandb_params_ft_job.id\n",
    "    print(f\"Starting to monitor fine-tuning job with ID: {job_id}\")\n",
    "\n",
    "    # Check the fine-tuning status until completion or failure\n",
    "    final_job = check_fine_tuning_status(client, job_id)\n",
    "    \n",
    "    # Print the final job details\n",
    "    print(\"\\nFinal job details:\")\n",
    "    print(f\"Status: {final_job.status}\")\n",
    "    print(f\"Created at: {final_job.created_at}\")\n",
    "    print(f\"Finished at: {final_job.finished_at}\")\n",
    "    print(f\"Fine-tuned model: {final_job.fine_tuned_model}\")\n",
    "    # You can add more job details here if needed\n",
    "\n",
    "except AttributeError:\n",
    "    print(\"Error: 'job' object does not have 'id' attribute. \"\n",
    "        \"Make sure the job was created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Fine-tuning status check process completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Over to Weights and Biases\n",
    "\n",
    "https://wandb.ai/home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "### Delete All Fine-Tuning Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all our fine-tuning jobs\n",
    "ft_jobs_list = client.fine_tuning.jobs.list()\n",
    "\n",
    "# Filter the list to only include jobs with a status of \"succeeded\"\n",
    "succeeded_jobs_list = [job for job in ft_jobs_list.data if job.status == \"succeeded\"]\n",
    "\n",
    "print(\"There are a total of \" + str(len(succeeded_jobs_list)) + \" successful jobs\\n\")\n",
    "\n",
    "print(\"===== Successful Jobs =====\")\n",
    "for job in succeeded_jobs_list:\n",
    "    print(job.id)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Loop through all succeeded jobs and mark the models for deletion if they exist\n",
    "# If a model doesn't exist then log it and continue to the next job\n",
    "# Most errors will be to delete models that don't exist because they have already been marked for deletion\n",
    "# NOTE: It can take a very long time for a model to be deleted after it has been marked for deletion\n",
    "# for job in succeeded_jobs_list:\n",
    "#     try:\n",
    "#         response = client.models.delete(job.fine_tuned_model)\n",
    "#         print(f\"Deleted model: {job.fine_tuned_model}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to delete model {job.fine_tuned_model}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NormalProgramming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
