{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning OpenAI API GPT Models for Sentiment Analysis With Weights & Biases\n",
    "\n",
    "### https://wandb.ai/mostafaibrahim17/ml-articles/reports/Fine-Tuning-ChatGPT-for-Sentiment-Analysis-With-W-B--Vmlldzo1NjMzMjQx \n",
    "\n",
    "This notebook explores fine-tuning GPT Models for sentiment analysis using Weights & Biases. Our experiment will lead to an overall accuracy boost, and we'll delve into applications. In today's data-driven world, sentiment analysis plays a pivotal role in discerning public opinion on a myriad of topics. Advanced models like GPT Model, built on the GPT architecture, offer immense potential in understanding and interpreting human emotions from textual data. However, like many tools, their out-of-the-box capabilities might not capture the nuanced intricacies of sentiment, especially in diverse datasets like those from Reddit. This article dives deep into the process of fine-tuning GPT Models for sentiment analysis, utilizing the powerful features of the Weights & Biases platform, and delves into the improvements and challenges faced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- How Can a GPT Model Be Used for Sentiment Analysis?\n",
    "- Fine-Tuning GPT Models for Sentiment Analysis\n",
    "- Data Preparation and Labeling\n",
    "  - The Current Data Set at Hand\n",
    "  - Data Augmentation Sentiment Analysis Dataset for Fine-Tuning\n",
    "  - The Importance of High-Quality Training Data for Sentiment Analysis\n",
    "- Step-by-Step Tutorial\n",
    "  - Evaluating the Old Model’s Performance\n",
    "  - Fine-Tuning the GPT Model\n",
    "  - Evaluating the New Model’s Performance\n",
    "- Fine-Tuning Results and Analysis\n",
    "- Practical Applications and Use Cases\n",
    "  - Jargon and Slang Understanding\n",
    "  - E-Commerce Product Reviews\n",
    "- Further Improvements\n",
    "- Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Can a GPT Model Be Used for Sentiment Analysis?\n",
    "\n",
    "GPT Model's ability to understand natural language makes it a good fit for sentiment analysis. This is because, unlike traditional chatbots that rely on predefined responses, GPT Models generate real-time answers based on a vast amount of training data. This approach enables it to provide responses that are contextually relevant and informed by a broad spectrum of information. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning GPT Model for Sentiment Analysis\n",
    "\n",
    "Fine-tuning is a pivotal step in adapting a general-purpose models, like GPT Models, to a specific task such as sentiment analysis. A GPT Model, with its broad language understanding capabilities, can grasp a vast array of topics and concepts. However, sentiment analysis is more than just comprehending text; it requires a nuanced understanding of subjective tones, moods, and emotions.\n",
    "<br/><br/>\n",
    "Think about sarcasm. Understanding sarcasm is tricky, even for humans sometimes. Sarcasm is when we say something but mean the opposite, often in a joking or mocking way. For example, if it starts raining just as you're about to go outside, and you say, \"Oh, perfect timing!\" you're probably being sarcastic because it's actually bad timing. Now, imagine a machine trying to understand this. Without special training, it might think you're genuinely happy about the rain because you said \"perfect.\" This is where fine-tuning a model like GPT Model becomes crucial.\n",
    "<br/><br/>\n",
    "GPT Model, out of the box, is pretty good at understanding a lot of text. It's read more than most humans ever will. But sarcasm is subtle and often needs context. So, to make GPT Models really get sarcasm, we'd expose it to many examples of sarcastic sentences until it starts catching on to the patterns. But here's the catch: sarcasm doesn't look the same everywhere. In different cultures or situations, what's sarcastic in one place might be meant seriously in another. That's why just general knowledge isn't enough. The model needs specific examples to truly grasp the playful twists and turns of sarcasm. In short, to make GPT Model understand sarcasm like a human, it needs extra training on it, just like someone might need to watch several comedy shows to start understanding a comedian's sense of humor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Labeling\n",
    "\n",
    "### The Current Data Set at Hand\n",
    "\n",
    "In this notebook, we'll leverage a Reddit dataset sourced from Kaggle, available here: https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset. This dataset features two key columns: clean_comment(the sentiment text) and its corresponding category (sentiment label).\n",
    "<br/><br/>\n",
    "The File Contains 37k comments along with its Sentimental Labelling. All the Comments in the dataset are cleaned and assigned with a Sentiment Label. These Comments Dataset Can Be used to Build a Sentimental Analysis Machine Learning Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Sentiment Analysis Dataset for Fine-Tuning\n",
    "\n",
    "It's important to note that the refined Fine-Tuning GPT Model process mandates a specific data structure in a JSONL file for optimal training. \n",
    "\n",
    "#### What is a JSONL File?\n",
    "\n",
    "A `.jsonl` file (short for JSON Lines) is a file format used to store structured data, typically for machine learning and data processing applications. Each line in a `.jsonl` file is a separate, self-contained JSON object. This makes it particularly useful for handling large datasets that can be processed line-by-line, avoiding loading everything into memory at once.\n",
    "\n",
    "##### Key Features of JSONL Format:\n",
    "- **One JSON Object Per Line:** Each line in the file is an independent JSON object.\n",
    "- **Line-Delimited:** The objects are separated by newlines (`\\n`), not by commas or brackets as in standard JSON.\n",
    "- **Efficient Parsing:** Line-by-line processing is easy and efficient, which is helpful when working with large datasets.\n",
    "- **No Root Structure:** Unlike regular JSON, there is no outer array or object enclosing the entire dataset.\n",
    "\n",
    "##### Example of a JSONL File:\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "\n",
    "\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "\n",
    "```\n",
    "##### Common Use Cases:\n",
    "- **Training Data for Machine Learning Models:** Frequently used in NLP tasks where each line contains an individual record (e.g., a sentence with a label).\n",
    "- **Log Data Storage:** Each log entry is a separate JSON object.\n",
    "- **Streaming Data Processing:** Ideal for scenarios where you process data incrementally.\n",
    "\n",
    "##### How to Work with JSONL:\n",
    "- **Reading and Writing:** In Python, you can use the `json` or `jsonlines` library to read and write JSONL files.\n",
    "- **Tools:** Many tools like `jq`, Pandas, and other data processing libraries support the JSONL format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Importance of High-Quality Training Data for Sentiment Analysis\n",
    "\n",
    "High-quality training data is pivotal for sentiment analysis as it ensures the model learns to accurately distinguish nuances in emotions. Poor data can lead to misinterpretations, reducing the effectiveness of the analysis. Moreover, comprehensive and well-curated data can significantly boost the model's ability to generalize across diverse real-world scenarios. The dataset we're utilizing underscores this point. As even some of its entries are so nuanced that even humans might struggle to discern their sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Tutorial\n",
    "\n",
    "### Evaluating the Normal Model's Performance\n",
    "\n",
    "#### Step 1: Installing and Importing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# The following commands are used to install the required libraries if they are not already installed.\n",
    "# Note: The exclamation mark (!) is used to run shell commands within a Jupyter Notebook or certain IDEs.\n",
    "\n",
    "# Uncomment and run the following lines if you need to install the libraries:\n",
    "# !pip install openai  # Install the OpenAI library for interacting with the OpenAI API\n",
    "# !pip install wandb   # Install the Weights and Biases (wandb) library for experiment tracking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os  # Provides a way to interact with the operating system\n",
    "import json  # For parsing and working with JSON data\n",
    "import random  # For generating random numbers and making random choices\n",
    "import datetime # For handling date and time-related functions\n",
    "from datetime import datetime\n",
    "import time  # For handling time-related functions\n",
    "from pathlib import Path  # For handling filesystem paths in an object-oriented way\n",
    "\n",
    "# Import third-party libraries\n",
    "import openai  # For accessing OpenAI’s GPT models\n",
    "from openai import OpenAI # For accessing OpenAI’s GPT models\n",
    "import wandb  # For experiment tracking and model logging with Weights and Biases\n",
    "import pandas as pd  # For handling data structures and data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Creating our client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the OpenAI client\n",
    "# This client will be used to interact with OpenAI's API\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Loading and Processing the Sentiment Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 37249 rows before cleaning.\n",
      "The DataFrame has 37149 rows after cleaning.\n",
      "                                       clean_comment  category\n",
      "0   family mormon have never tried explain them t...         1\n",
      "1  buddhism has very much lot compatible with chr...         1\n",
      "2  seriously don say thing first all they won get...        -1\n",
      "3  what you have learned yours and only yours wha...         0\n",
      "4  for your own benefit you may want read living ...         1\n"
     ]
    }
   ],
   "source": [
    "# Define the file path for the CSV data\n",
    "filename = \"./practical_data/reddit_data.csv\"\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "#Get the total number of rows and columns in the DataFrame before cleaning\n",
    "num_rows = len(df)\n",
    "print(f\"The DataFrame has {num_rows} rows before cleaning.\")\n",
    "\n",
    "# Remove rows with missing values in 'clean_comment' and 'category' columns\n",
    "# The 'inplace=True' argument modifies the DataFrame directly\n",
    "df.dropna(subset=['clean_comment', 'category'], inplace=True)\n",
    "\n",
    "#Get the total number of rows and columns in the DataFrame after cleaning\n",
    "num_rows = len(df)\n",
    "print(f\"The DataFrame has {num_rows} rows after cleaning.\\n\")\n",
    "\n",
    "# Display the first 5 rows of the DataFrame\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Initializing a New Weights & Biases Project\n",
    "\n",
    "Now for something new. We will create a new WandB project with code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msuspicious-cow\u001b[0m (\u001b[33msuspicious-cow-self\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Zain_\\Dropbox\\Personal\\Data Science Projects\\OpenAI_API_Fine_Tuning\\wandb\\run-20240818_110242-a1tpxe16</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning/runs/a1tpxe16' target=\"_blank\">elated-cosmos-1</a></strong> to <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning' target=\"_blank\">https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning/runs/a1tpxe16' target=\"_blank\">https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning/runs/a1tpxe16</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the environment variable for the notebook name\n",
    "# This helps Weights & Biases (wandb) identify the source of the run\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"fine_tune_openai_sentiment_wandb.ipynb\"\n",
    "\n",
    "# Initialize a new Weights & Biases run\n",
    "# This creates a new experiment in the \"Reddit_Sentiment_Analysis\" project\n",
    "run = wandb.init(project=\"Reddit_Sentiment_Analysis_Fine_Tuning\")\n",
    "\n",
    "# Note: Remember to close the wandb run when you're finished\n",
    "# Uncomment the following line at the end of your script:\n",
    "# run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Take a Sample to Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select a sample of 100 rows from the DataFrame\n",
    "df_sample = df.sample(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Define Helper Functions to Convert Model Response to Sentiment Value and Vice Versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_response_to_sentiment(response):\n",
    "    \"\"\"\n",
    "    Convert a text response to a numeric sentiment value.\n",
    "\n",
    "    Args:\n",
    "    response (str): The sentiment response as text.\n",
    "\n",
    "    Returns:\n",
    "    int: The sentiment as a numeric value:\n",
    "        1 for positive, -1 for negative, 0 for neutral, -1 for unknown.\n",
    "    \"\"\"\n",
    "    response = response.lower()\n",
    "    if 'positive' in response:\n",
    "        return 1\n",
    "    elif 'negative' in response:\n",
    "        return -1\n",
    "    elif 'neutral' in response:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1  # Unknown sentiment\n",
    "\n",
    "\n",
    "def convert_numeric_to_string_sentiment(value):\n",
    "    \"\"\"\n",
    "    Convert a numeric sentiment value to a string representation.\n",
    "\n",
    "    Args:\n",
    "    value (int): The sentiment as a numeric value.\n",
    "\n",
    "    Returns:\n",
    "    str: The sentiment as a string:\n",
    "        \"positive\", \"negative\", \"neutral\", or \"unknown\".\n",
    "    \"\"\"\n",
    "    if value == 1:\n",
    "        return \"positive\"\n",
    "    elif value == -1:\n",
    "        return \"negative\"\n",
    "    elif value == 0:\n",
    "        return \"neutral\"\n",
    "    else:\n",
    "        return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluating the Current Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/100 rows.\n",
      "Processed 2/100 rows.\n",
      "Processed 3/100 rows.\n",
      "Processed 4/100 rows.\n",
      "Processed 5/100 rows.\n",
      "Processed 6/100 rows.\n",
      "Processed 7/100 rows.\n",
      "Processed 8/100 rows.\n",
      "Processed 9/100 rows.\n",
      "Processed 10/100 rows.\n",
      "Processed 11/100 rows.\n",
      "Processed 12/100 rows.\n",
      "Processed 13/100 rows.\n",
      "Processed 14/100 rows.\n",
      "Processed 15/100 rows.\n",
      "Processed 16/100 rows.\n",
      "Processed 17/100 rows.\n",
      "Processed 18/100 rows.\n",
      "Processed 19/100 rows.\n",
      "Processed 20/100 rows.\n",
      "Processed 21/100 rows.\n",
      "Processed 22/100 rows.\n",
      "Processed 23/100 rows.\n",
      "Processed 24/100 rows.\n",
      "Processed 25/100 rows.\n",
      "Processed 26/100 rows.\n",
      "Processed 27/100 rows.\n",
      "Processed 28/100 rows.\n",
      "Processed 29/100 rows.\n",
      "Processed 30/100 rows.\n",
      "Processed 31/100 rows.\n",
      "Processed 32/100 rows.\n",
      "Processed 33/100 rows.\n",
      "Processed 34/100 rows.\n",
      "Processed 35/100 rows.\n",
      "Processed 36/100 rows.\n",
      "Processed 37/100 rows.\n",
      "Processed 38/100 rows.\n",
      "Processed 39/100 rows.\n",
      "Processed 40/100 rows.\n",
      "Processed 41/100 rows.\n",
      "Processed 42/100 rows.\n",
      "Processed 43/100 rows.\n",
      "Processed 44/100 rows.\n",
      "Processed 45/100 rows.\n",
      "Processed 46/100 rows.\n",
      "Processed 47/100 rows.\n",
      "Processed 48/100 rows.\n",
      "Processed 49/100 rows.\n",
      "Processed 50/100 rows.\n",
      "Processed 51/100 rows.\n",
      "Processed 52/100 rows.\n",
      "Processed 53/100 rows.\n",
      "Processed 54/100 rows.\n",
      "Processed 55/100 rows.\n",
      "Processed 56/100 rows.\n",
      "Processed 57/100 rows.\n",
      "Processed 58/100 rows.\n",
      "Processed 59/100 rows.\n",
      "Processed 60/100 rows.\n",
      "Processed 61/100 rows.\n",
      "Processed 62/100 rows.\n",
      "Processed 63/100 rows.\n",
      "Processed 64/100 rows.\n",
      "Processed 65/100 rows.\n",
      "Processed 66/100 rows.\n",
      "Processed 67/100 rows.\n",
      "Processed 68/100 rows.\n",
      "Processed 69/100 rows.\n",
      "Processed 70/100 rows.\n",
      "Processed 71/100 rows.\n",
      "Processed 72/100 rows.\n",
      "Processed 73/100 rows.\n",
      "Processed 74/100 rows.\n",
      "Processed 75/100 rows.\n",
      "Processed 76/100 rows.\n",
      "Processed 77/100 rows.\n",
      "Processed 78/100 rows.\n",
      "Processed 79/100 rows.\n",
      "Processed 80/100 rows.\n",
      "Processed 81/100 rows.\n",
      "Processed 82/100 rows.\n",
      "Processed 83/100 rows.\n",
      "Processed 84/100 rows.\n",
      "Processed 85/100 rows.\n",
      "Processed 86/100 rows.\n",
      "Processed 87/100 rows.\n",
      "Processed 88/100 rows.\n",
      "Processed 89/100 rows.\n",
      "Processed 90/100 rows.\n",
      "Processed 91/100 rows.\n",
      "Processed 92/100 rows.\n",
      "Processed 93/100 rows.\n",
      "Processed 94/100 rows.\n",
      "Processed 95/100 rows.\n",
      "Processed 96/100 rows.\n",
      "Processed 97/100 rows.\n",
      "Processed 98/100 rows.\n",
      "Processed 99/100 rows.\n",
      "Processed 100/100 rows.\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters and results list\n",
    "correct_predictions = 0\n",
    "loop_count = 0\n",
    "results = []\n",
    "\n",
    "# Iterate over each row in the sample DataFrame\n",
    "for index, row in df_sample.iterrows():\n",
    "    loop_count += 1\n",
    "    text = row['clean_comment']\n",
    "    \n",
    "    try:\n",
    "        # Make an API call to OpenAI for sentiment analysis\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"What is the sentiment of the following text? Please respond with 'positive', 'negative', or 'neutral'.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the model's response\n",
    "        response = completion.choices[0].message.content\n",
    "        \n",
    "        # Convert the text response to a numeric sentiment\n",
    "        predicted_sentiment = convert_response_to_sentiment(response)\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            \"sentiment\": text,  \n",
    "            \"labeled_prediction\": convert_numeric_to_string_sentiment(row['category']),\n",
    "            \"old_model_prediction\": response\n",
    "        })\n",
    "        \n",
    "        # Check if the prediction matches the actual sentiment\n",
    "        if predicted_sentiment == row['category']:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        # Print progress\n",
    "        total_rows = len(df_sample)\n",
    "        print(f\"Processed {loop_count}/{total_rows} rows.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on index {index}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8: Calculating the Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy percentage\n",
    "accuracy = (correct_predictions / total_rows) * 100\n",
    "\n",
    "# Print the accuracy with two decimal places\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9: Logging the Accuracy to WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy before: 42.00%\n"
     ]
    }
   ],
   "source": [
    "# Log the accuracy to Weights & Biases\n",
    "wandb.log({\"Old Accuracy\": accuracy})\n",
    "\n",
    "# Print the accuracy to the console\n",
    "print(f'Model Accuracy before: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Model\n",
    "\n",
    "#### Step 10: Converting the Dataframe to JSONL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to reddit_sentiment_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_filename = \"reddit_sentiment_data.jsonl\"\n",
    "\n",
    "# Convert DataFrame to the desired JSONL format\n",
    "with open(output_filename, \"w\") as file:\n",
    "    for _, row in df.iterrows():\n",
    "        # Map the numeric sentiment to its corresponding string label\n",
    "        target_label = {\n",
    "            0: 'neutral',\n",
    "            1: 'positive',\n",
    "            -1: 'negative'\n",
    "        }.get(row['category'], 'unknown')\n",
    "        \n",
    "        # Create a dictionary representing the conversation format\n",
    "        data = {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"What is the sentiment of the following text? Please respond with 'positive', 'negative', or 'neutral'.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": row['clean_comment']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": target_label\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Write each data point as a separate line in the JSONL file\n",
    "        file.write(json.dumps(data) + \"\\n\")\n",
    "\n",
    "print(f\"Data has been written to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 10b: Create the Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for doing train and test splits on JSONL files\n",
    "def split_jsonl_file(file_path, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Split a JSONL file into training and test sets.\n",
    "\n",
    "    Args:\n",
    "    file_path (str): Path to the input JSONL file.\n",
    "    train_ratio (float): Ratio of data to use for training (default: 0.8).\n",
    "\n",
    "    Returns:\n",
    "    tuple: Paths to the created training and test files.\n",
    "    \"\"\"\n",
    "    # Convert file path to Path object\n",
    "    file_path = Path(file_path)\n",
    "\n",
    "    # Read and parse the input JSONL file\n",
    "    with file_path.open('r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    # Shuffle the data randomly\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculate the split index\n",
    "    split_index = int(len(data) * train_ratio)\n",
    "    \n",
    "    # Split the data into train and test sets\n",
    "    train_data = data[:split_index]\n",
    "    test_data = data[split_index:]\n",
    "    \n",
    "    # Prepare output file paths\n",
    "    train_file = file_path.with_name(f\"{file_path.stem}_train{file_path.suffix}\")\n",
    "    test_file = file_path.with_name(f\"{file_path.stem}_test{file_path.suffix}\")\n",
    "    \n",
    "    # Write train data to file\n",
    "    with train_file.open('w', encoding='utf-8') as f:\n",
    "        for item in train_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # Write test data to file\n",
    "    with test_file.open('w', encoding='utf-8') as f:\n",
    "        for item in test_data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    # Print summary information\n",
    "    print(f\"Train data saved to: {train_file}\")\n",
    "    print(f\"Test data saved to: {test_file}\")\n",
    "    print(f\"Train set size: {len(train_data)}\")\n",
    "    print(f\"Test set size: {len(test_data)}\")\n",
    "    \n",
    "    return train_file, test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data saved to: reddit_sentiment_data_train.jsonl\n",
      "Test data saved to: reddit_sentiment_data_test.jsonl\n",
      "Train set size: 29719\n",
      "Test set size: 7430\n",
      "\n",
      "\n",
      "Train file path: reddit_sentiment_data_train.jsonl\n",
      "Test file path: reddit_sentiment_data_test.jsonl\n"
     ]
    }
   ],
   "source": [
    "# File paths and data processing\n",
    "file_path = output_filename\n",
    "\n",
    "# Split the JSONL file into train and test sets\n",
    "train_test_files = split_jsonl_file(file_path)\n",
    "print(\"\\n\")  # Print a blank line for better output readability\n",
    "\n",
    "# Convert the returned file paths to strings\n",
    "train_path, test_path = [str(file) for file in train_test_files]\n",
    "\n",
    "# Print the paths of the resulting train and test files\n",
    "print(f\"Train file path: {train_path}\")\n",
    "print(f\"Test file path: {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 11: Upload the files to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file uploaded with ID: file-y5CZr6MnLF8IAWsAH5p88muT\n",
      "Test file uploaded with ID: file-6OUJ7W7T83e1JGkAJkO3DVwe\n"
     ]
    }
   ],
   "source": [
    "# Upload the training data to the OpenAI API\n",
    "train_set_file = client.files.create(\n",
    "    file=open(train_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# Upload the test data to the OpenAI API\n",
    "test_set_file = client.files.create(\n",
    "    file=open(test_path, \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# Print confirmation messages\n",
    "print(f\"Training file uploaded with ID: {train_set_file.id}\")\n",
    "print(f\"Test file uploaded with ID: {test_set_file.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12: Create a Fine-Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job created with ID: ftjob-NDiUitRsbGkDI7D6XPFLI0cD\n",
      "Model: gpt-3.5-turbo-0125\n",
      "Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "# Create a fine-tuning job using the uploaded training data\n",
    "wandb_params_ft_job = client.fine_tuning.jobs.create(\n",
    "    model=\"gpt-3.5-turbo\",  # Base model to be fine-tuned\n",
    "    training_file=train_set_file.id,  # ID of the uploaded training data file\n",
    "    validation_file=test_set_file.id,  # ID of the uploaded validation (test) data file\n",
    "    hyperparameters={\n",
    "        \"batch_size\": \"auto\",  # Let API automatically determine batch size\n",
    "        \"learning_rate_multiplier\": \"auto\",  # Auto-set learning rate multiplier\n",
    "        \"n_epochs\": \"auto\",  # Automatically decide number of training epochs\n",
    "    },\n",
    "    suffix=\"reddit_sentiment\",  # Append this to the fine-tuned model's name\n",
    "    integrations=[\n",
    "        {\n",
    "            \"type\": \"wandb\",\n",
    "            \"wandb\": {\n",
    "                \"project\": \"Reddit_Sentiment_Analysis\",  # Replace with your actual project name\n",
    "                \"name\": \"Reddit_Sentiment_Analysis_Run_001\",  # Optional: Replace with your desired run name\n",
    "                \"entity\": \"suspicious-cow-self\",  # Optional: Replace with your entity\n",
    "                \"tags\": [\"reddit\", \"sentiment\"]  # Optional: Replace with your desired tags\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    seed=None,  # Specific random seed set for reproducibility\n",
    ")\n",
    "\n",
    "# Print confirmation and job details\n",
    "print(f\"Fine-tuning job created with ID: {wandb_params_ft_job.id}\")\n",
    "print(f\"Model: {wandb_params_ft_job.model}\")\n",
    "print(f\"Status: {wandb_params_ft_job.status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 12a: Make Sure the Job is Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create helper function to check the status of a fine-tuning job\n",
    "# Along with a special exception for failed jobs\n",
    "class FineTuningFailedException(Exception):\n",
    "    \"\"\"Custom exception for failed fine-tuning jobs.\"\"\"\n",
    "    pass\n",
    "\n",
    "def check_fine_tuning_status(client: OpenAI, job_id: str, seconds_to_wait: int = 60) -> dict:\n",
    "    \"\"\"\n",
    "    Continuously check the status of a fine-tuning job until it succeeds or fails.\n",
    "\n",
    "    Args:\n",
    "        client (OpenAI): The OpenAI API client object.\n",
    "        job_id (str): The ID of the fine-tuning job to check.\n",
    "        seconds_to_wait (int): The number of seconds to wait between status checks.\n",
    "\n",
    "    Returns:\n",
    "        dict: The final job details if the job succeeds.\n",
    "\n",
    "    Raises:\n",
    "        FineTuningFailedException: If the fine-tuning job fails.\n",
    "        Exception: For any other errors during the process.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            # Retrieve updated information for the fine-tuning job\n",
    "            retrieved_job = client.fine_tuning.jobs.retrieve(job_id)\n",
    "            \n",
    "            print(f\"Current status: {retrieved_job.status}\")\n",
    "            \n",
    "            if retrieved_job.status == \"failed\":\n",
    "                print(\"Job failed. Final job details:\")\n",
    "                print(retrieved_job)\n",
    "                raise FineTuningFailedException(\"The fine-tuning job has failed.\")\n",
    "            \n",
    "            if retrieved_job.status == \"succeeded\":\n",
    "                print(\"Job succeeded. Final job details:\")\n",
    "                print(retrieved_job)\n",
    "                return retrieved_job\n",
    "            \n",
    "            # Wait for the specified number of seconds before checking again\n",
    "            time.sleep(seconds_to_wait)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            raise  # Re-raise the exception to stop the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to monitor fine-tuning job with ID: ftjob-NDiUitRsbGkDI7D6XPFLI0cD\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: validating_files\n",
      "Current status: running\n",
      "Current status: running\n",
      "Current status: running\n",
      "Current status: running\n",
      "Current status: running\n",
      "Current status: running\n",
      "Current status: succeeded\n",
      "Job succeeded. Final job details:\n",
      "FineTuningJob(id='ftjob-NDiUitRsbGkDI7D6XPFLI0cD', created_at=1723997733, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-3.5-turbo-0125:personal:reddit-sentiment:9xewUqje', finished_at=1724005760, hyperparameters=Hyperparameters(n_epochs=1, batch_size=19, learning_rate_multiplier=2), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-SQH2HT1IvRszon9pdYwV1yvQ', result_files=['file-caEVDcydgUhYKNuwWJyLFNRk'], seed=1130407589, status='succeeded', trained_tokens=2133448, training_file='file-y5CZr6MnLF8IAWsAH5p88muT', validation_file='file-6OUJ7W7T83e1JGkAJkO3DVwe', estimated_finish=None, integrations=[FineTuningJobWandbIntegrationObject(type='wandb', wandb=FineTuningJobWandbIntegration(project='Reddit_Sentiment_Analysis', entity='suspicious-cow-self', name=None, tags=None, run_id='ftjob-NDiUitRsbGkDI7D6XPFLI0cD'))], user_provided_suffix='reddit_sentiment')\n",
      "\n",
      "Final job details:\n",
      "Status: succeeded\n",
      "Error: 'job' object does not have 'id' attribute. Make sure the job was created successfully.\n",
      "Fine-tuning status check process completed.\n"
     ]
    }
   ],
   "source": [
    "# Use our helper function to make sure the job is done\n",
    "# Then print the final job details\n",
    "try:\n",
    "    # Extract the job ID from the previously created fine-tuning job\n",
    "    job_id = wandb_params_ft_job.id\n",
    "    \n",
    "    print(f\"Starting to monitor fine-tuning job with ID: {job_id}\")\n",
    "\n",
    "    # Check the fine-tuning status until completion or failure\n",
    "    # Set the checking interval to 10 minutes (600 seconds)\n",
    "    final_job = check_fine_tuning_status(client, job_id, seconds_to_wait=600)\n",
    "    \n",
    "    \n",
    "except AttributeError:\n",
    "    print(\"Error: 'job' object does not have 'id' attribute. \"\n",
    "        \"Make sure the job was created successfully.\")\n",
    "\n",
    "except openai.OpenAIError as e:\n",
    "    print(f\"An OpenAI API error occurred: {str(e)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    print(\"Fine-tuning status check process completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final job details:\n",
      "Status: succeeded\n",
      "Created at: 1723997733\n",
      "Finished at: 1724005760\n",
      "Fine-tuned model: ft:gpt-3.5-turbo-0125:personal:reddit-sentiment:9xewUqje\n",
      "Training file: file-y5CZr6MnLF8IAWsAH5p88muT\n",
      "Validation file: file-6OUJ7W7T83e1JGkAJkO3DVwe\n",
      "Trained tokens: 2133448\n"
     ]
    }
   ],
   "source": [
    "# Print the final job details\n",
    "print(\"\\nFinal job details:\")\n",
    "print(f\"Status: {final_job.status}\")\n",
    "print(f\"Created at: {final_job.created_at}\")\n",
    "print(f\"Finished at: {final_job.finished_at}\")\n",
    "print(f\"Fine-tuned model: {final_job.fine_tuned_model}\")\n",
    "print(f\"Training file: {final_job.training_file}\")\n",
    "print(f\"Validation file: {final_job.validation_file}\")\n",
    "print(f\"Trained tokens: {final_job.trained_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created at: 2024-08-18 06:15:33 CDT\n",
      "Finished at: 2024-08-18 08:29:20 CDT\n"
     ]
    }
   ],
   "source": [
    "# Let's show friendly timestamps for easier reading\n",
    "import pytz\n",
    "\n",
    "# Set the timezone to US Central Time\n",
    "central_tz = pytz.timezone('US/Central')\n",
    "\n",
    "def format_timestamp(timestamp):\n",
    "    # Convert Unix timestamp to datetime object in UTC\n",
    "    dt_utc = datetime.fromtimestamp(timestamp).replace(tzinfo=pytz.UTC)\n",
    "    \n",
    "    # Convert to Central Time\n",
    "    dt_central = dt_utc.astimezone(central_tz)\n",
    "    \n",
    "    # Format the datetime object as a string\n",
    "    return dt_central.strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "# Assuming final_job.created_at and final_job.finished_at are your Unix timestamps\n",
    "print(f\"Created at: {format_timestamp(final_job.created_at)}\")\n",
    "print(f\"Finished at: {format_timestamp(final_job.finished_at)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model name: ft:gpt-3.5-turbo-0125:personal:reddit-sentiment:9xewUqje\n",
      "Job status: succeeded\n",
      "Training file: file-y5CZr6MnLF8IAWsAH5p88muT\n",
      "Validation file: file-6OUJ7W7T83e1JGkAJkO3DVwe\n",
      "Created at: 1723997733\n",
      "Finished at: 1724005760\n",
      "Trained tokens: 2133448\n"
     ]
    }
   ],
   "source": [
    "# Get the final status of the fine-tuning job\n",
    "wandb_params_ft_job_final = client.fine_tuning.jobs.retrieve(wandb_params_ft_job.id)\n",
    "\n",
    "# Print the name of the fine-tuned model\n",
    "print(f\"Fine-tuned model name: {wandb_params_ft_job_final.fine_tuned_model}\")\n",
    "\n",
    "# Optional: Print additional job details\n",
    "print(f\"Job status: {wandb_params_ft_job_final.status}\")\n",
    "print(f\"Training file: {wandb_params_ft_job_final.training_file}\")\n",
    "print(f\"Validation file: {wandb_params_ft_job_final.validation_file}\")\n",
    "print(f\"Created at: {wandb_params_ft_job_final.created_at}\")\n",
    "print(f\"Finished at: {wandb_params_ft_job_final.finished_at}\")\n",
    "print(f\"Trained tokens: {wandb_params_ft_job_final.trained_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the New Model's Performance\n",
    "\n",
    "#### Step 13: Looking at the metrics from WandB\n",
    "\n",
    "We will do manual calculations later for fun but, for now, let's look at the data from WandB. There are two ways you can do this:\n",
    "1. Through the WandB website\n",
    "2. Through code\n",
    "<br/><br/>\n",
    "We will do both of these. \n",
    "\n",
    "First, for the website go to https://wandb.ai/suspicious-cow-self/projects and click on the Reddit_Sentiment_Analysis project. This should automatically show you the results from the latest run in a graphical format. \n",
    "\n",
    "Second, let's manually compute rough statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/100 rows.\n",
      "Processed 2/100 rows.\n",
      "Processed 3/100 rows.\n",
      "Processed 4/100 rows.\n",
      "Processed 5/100 rows.\n",
      "Processed 6/100 rows.\n",
      "Processed 7/100 rows.\n",
      "Processed 8/100 rows.\n",
      "Processed 9/100 rows.\n",
      "Processed 10/100 rows.\n",
      "Processed 11/100 rows.\n",
      "Processed 12/100 rows.\n",
      "Processed 13/100 rows.\n",
      "Processed 14/100 rows.\n",
      "Processed 15/100 rows.\n",
      "Processed 16/100 rows.\n",
      "Processed 17/100 rows.\n",
      "Processed 18/100 rows.\n",
      "Processed 19/100 rows.\n",
      "Processed 20/100 rows.\n",
      "Processed 21/100 rows.\n",
      "Processed 22/100 rows.\n",
      "Processed 23/100 rows.\n",
      "Processed 24/100 rows.\n",
      "Processed 25/100 rows.\n",
      "Processed 26/100 rows.\n",
      "Processed 27/100 rows.\n",
      "Processed 28/100 rows.\n",
      "Processed 29/100 rows.\n",
      "Processed 30/100 rows.\n",
      "Processed 31/100 rows.\n",
      "Processed 32/100 rows.\n",
      "Processed 33/100 rows.\n",
      "Processed 34/100 rows.\n",
      "Processed 35/100 rows.\n",
      "Processed 36/100 rows.\n",
      "Processed 37/100 rows.\n",
      "Processed 38/100 rows.\n",
      "Processed 39/100 rows.\n",
      "Processed 40/100 rows.\n",
      "Processed 41/100 rows.\n",
      "Processed 42/100 rows.\n",
      "Processed 43/100 rows.\n",
      "Processed 44/100 rows.\n",
      "Processed 45/100 rows.\n",
      "Processed 46/100 rows.\n",
      "Processed 47/100 rows.\n",
      "Processed 48/100 rows.\n",
      "Processed 49/100 rows.\n",
      "Processed 50/100 rows.\n",
      "Processed 51/100 rows.\n",
      "Processed 52/100 rows.\n",
      "Processed 53/100 rows.\n",
      "Processed 54/100 rows.\n",
      "Processed 55/100 rows.\n",
      "Processed 56/100 rows.\n",
      "Processed 57/100 rows.\n",
      "Processed 58/100 rows.\n",
      "Processed 59/100 rows.\n",
      "Processed 60/100 rows.\n",
      "Processed 61/100 rows.\n",
      "Processed 62/100 rows.\n",
      "Processed 63/100 rows.\n",
      "Processed 64/100 rows.\n",
      "Processed 65/100 rows.\n",
      "Processed 66/100 rows.\n",
      "Processed 67/100 rows.\n",
      "Processed 68/100 rows.\n",
      "Processed 69/100 rows.\n",
      "Processed 70/100 rows.\n",
      "Processed 71/100 rows.\n",
      "Processed 72/100 rows.\n",
      "Processed 73/100 rows.\n",
      "Processed 74/100 rows.\n",
      "Processed 75/100 rows.\n",
      "Processed 76/100 rows.\n",
      "Processed 77/100 rows.\n",
      "Processed 78/100 rows.\n",
      "Processed 79/100 rows.\n",
      "Processed 80/100 rows.\n",
      "Processed 81/100 rows.\n",
      "Processed 82/100 rows.\n",
      "Processed 83/100 rows.\n",
      "Processed 84/100 rows.\n",
      "Processed 85/100 rows.\n",
      "Processed 86/100 rows.\n",
      "Processed 87/100 rows.\n",
      "Processed 88/100 rows.\n",
      "Processed 89/100 rows.\n",
      "Processed 90/100 rows.\n",
      "Processed 91/100 rows.\n",
      "Processed 92/100 rows.\n",
      "Processed 93/100 rows.\n",
      "Processed 94/100 rows.\n",
      "Processed 95/100 rows.\n",
      "Processed 96/100 rows.\n",
      "Processed 97/100 rows.\n",
      "Processed 98/100 rows.\n",
      "Processed 99/100 rows.\n",
      "Processed 100/100 rows.\n"
     ]
    }
   ],
   "source": [
    "# Use our fine-tuned model to make predictions on the sample data\n",
    "model_id = wandb_params_ft_job_final.fine_tuned_model\n",
    "correct_predictions = 0\n",
    "loop_count = 0  # Counter for loop iterations\n",
    "loop_index = 0  # Initialize loop_index\n",
    "\n",
    "# Iterate over each row in the DataFrame for the new model\n",
    "for index, row in df_sample.iterrows():\n",
    "    loop_count += 1  # Increment the loop count\n",
    "    text = row['clean_comment']\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"What is the sentiment of the following text? Please respond with 'positive', 'negative', or 'neutral'.\"},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ]\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        predicted_sentiment = convert_response_to_sentiment(response)\n",
    "        \n",
    "        results[loop_index].update({\"new_model_prediction\": response})\n",
    "        loop_index += 1  # Increment the loop index\n",
    "        \n",
    "        # Check if the predicted sentiment matches the actual sentiment\n",
    "        if predicted_sentiment == row['category']:\n",
    "            correct_predictions += 1\n",
    "        \n",
    "        # Print the current progress\n",
    "        print(f\"Processed {loop_count}/{len(df_sample)} rows.\")\n",
    "        \n",
    "    except openai.OpenAIError as e:\n",
    "        print(f\"OpenAI API error on index {index}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error on index {index}: {e}\")\n",
    "    \n",
    "    # Optional: Add a delay to avoid hitting rate limits\n",
    "    # time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 14: Calculating the Fine-Tuned Model Accuracy Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New Model Accuracy: 87.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print accuracy\n",
    "accuracy = (correct_predictions / len(df_sample)) * 100\n",
    "print(f\"\\nNew Model Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 15: Logging the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy after fine-tuning: 87.00%\n",
      "Accuracy Improvement: 45.00 percentage points\n",
      "Relative Improvement: 107.14%\n"
     ]
    }
   ],
   "source": [
    "# Log the new accuracy to Weights & Biases\n",
    "wandb.log({\"New Model Accuracy\": accuracy})\n",
    "\n",
    "# Print the new accuracy to the console\n",
    "print(f'Model Accuracy after fine-tuning: {accuracy:.2f}%')\n",
    "\n",
    "# Optional: Log additional metrics or comparisons\n",
    "old_accuracy = wandb.run.summary.get(\"Old Accuracy\", 0)  # Get the old accuracy, default to 0 if not found\n",
    "accuracy_improvement = accuracy - old_accuracy\n",
    "\n",
    "wandb.log({\n",
    "    \"Accuracy Improvement\": accuracy_improvement,\n",
    "    \"Relative Improvement\": (accuracy_improvement / old_accuracy) * 100 if old_accuracy > 0 else 0\n",
    "})\n",
    "\n",
    "print(f'Accuracy Improvement: {accuracy_improvement:.2f} percentage points')\n",
    "print(f'Relative Improvement: {(accuracy_improvement / old_accuracy) * 100:.2f}%' if old_accuracy > 0 else 'N/A (no previous accuracy recorded)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 16: Create a Fine-Tuned vs Non-Tuned Result Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the results DataFrame:\n",
      "                                           sentiment labeled_prediction  \\\n",
      "0                      have words just how the does             neutral   \n",
      "1  what with stig abel shaking like leaf and hasn...            neutral   \n",
      "2                         may 2018 update don think             neutral   \n",
      "3  based indian leader talking about legitimate e...           negative   \n",
      "4   quote arun shourie when all said and done mor...           positive   \n",
      "\n",
      "  old_model_prediction new_model_prediction  \n",
      "0              neutral              neutral  \n",
      "1             negative              neutral  \n",
      "2              neutral              neutral  \n",
      "3             negative             positive  \n",
      "4             positive             positive  \n",
      "\n",
      "Summary Statistics:\n",
      "total_samples: 100\n",
      "unique_sentiments: 3\n",
      "old_model_accuracy: 0.42\n",
      "new_model_accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Convert results list to DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "print(\"First few rows of the results DataFrame:\")\n",
    "print(df_results.head())\n",
    "\n",
    "# Log the entire DataFrame as a table to Weights & Biases\n",
    "wandb.log({\"results_table\": wandb.Table(dataframe=df_results)})\n",
    "\n",
    "# Optional: Log some summary statistics\n",
    "summary_stats = {\n",
    "    \"total_samples\": len(df_results),\n",
    "    \"unique_sentiments\": df_results['labeled_prediction'].nunique(),\n",
    "    \"old_model_accuracy\": (df_results['old_model_prediction'] == df_results['labeled_prediction']).mean(),\n",
    "    \"new_model_accuracy\": (df_results['new_model_prediction'] == df_results['labeled_prediction']).mean()\n",
    "}\n",
    "\n",
    "wandb.log(summary_stats)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "for key, value in summary_stats.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Optional: Create and log a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_confusion_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['positive', 'neutral', 'negative'])\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['positive', 'neutral', 'negative'], yticklabels=['positive', 'neutral', 'negative'])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    return plt\n",
    "\n",
    "new_model_cm = create_confusion_matrix(df_results['labeled_prediction'], df_results['new_model_prediction'], 'New Model Confusion Matrix')\n",
    "wandb.log({\"new_model_confusion_matrix\": wandb.Image(new_model_cm)})\n",
    "\n",
    "old_model_cm = create_confusion_matrix(df_results['labeled_prediction'], df_results['old_model_prediction'], 'Old Model Confusion Matrix')\n",
    "wandb.log({\"old_model_confusion_matrix\": wandb.Image(old_model_cm)})\n",
    "\n",
    "plt.close('all')  # Close all plt figures to free up memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 17: Finish the WandB Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6e7da0a9194b6abb1bf0c9d3f794fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.108 MB of 0.142 MB uploaded\\r'), FloatProgress(value=0.7570944699688118, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy Improvement</td><td>▁</td></tr><tr><td>New Model Accuracy</td><td>▁</td></tr><tr><td>Old Accuracy</td><td>▁</td></tr><tr><td>Relative Improvement</td><td>▁</td></tr><tr><td>new_model_accuracy</td><td>▁</td></tr><tr><td>old_model_accuracy</td><td>▁</td></tr><tr><td>total_samples</td><td>▁</td></tr><tr><td>unique_sentiments</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy Improvement</td><td>45.0</td></tr><tr><td>New Model Accuracy</td><td>87.0</td></tr><tr><td>Old Accuracy</td><td>42.0</td></tr><tr><td>Relative Improvement</td><td>107.14286</td></tr><tr><td>new_model_accuracy</td><td>0.87</td></tr><tr><td>old_model_accuracy</td><td>0.42</td></tr><tr><td>total_samples</td><td>100</td></tr><tr><td>unique_sentiments</td><td>3</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">elated-cosmos-1</strong> at: <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning/runs/a1tpxe16' target=\"_blank\">https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning/runs/a1tpxe16</a><br/> View project at: <a href='https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning' target=\"_blank\">https://wandb.ai/suspicious-cow-self/Reddit_Sentiment_Analysis_Fine_Tuning</a><br/>Synced 6 W&B file(s), 3 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240818_110242-a1tpxe16\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights & Biases run has been completed and synced.\n"
     ]
    }
   ],
   "source": [
    "# Close the wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Weights & Biases run has been completed and synced.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
